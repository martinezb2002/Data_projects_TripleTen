{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is in conjunction with the mining company OilyGiant, with the hopes of finding the best location for a new well. With the help of a machine learning model, we will be able to determine which region will be most worthwhile. This will be determined by each region's quality, volume, and estimates. I will begin by preparing the data , then moving on to creating and training the model. The next step will be to calculate various metrics related to profit. With that information, I will be able to select the best region. That is not without the consideration of potential risks and errors, which will be identified with the help of the Bootstrapping technique. With the support from this project, OilyGiant will be able to have the location of a new well while also having the security knowing that it will produce the most for the best amount. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/datasets/geo_data_0.csv') \n",
    "df2 = pd.read_csv('/datasets/geo_data_1.csv')\n",
    "df3 = pd.read_csv('/datasets/geo_data_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count   Dtype  \n",
      "---  ------   --------------   -----  \n",
      " 0   id       100000 non-null  object \n",
      " 1   f0       100000 non-null  float64\n",
      " 2   f1       100000 non-null  float64\n",
      " 3   f2       100000 non-null  float64\n",
      " 4   product  100000 non-null  float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 3.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(          id        f0        f1        f2     product\n",
       " 0      txEyH  0.705745 -0.497823  1.221170  105.280062\n",
       " 1      2acmU  1.334711 -0.340164  4.365080   73.037750\n",
       " 2      409Wp  1.022732  0.151990  1.419926   85.265647\n",
       " 3      iJLyR -0.032172  0.139033  2.978566  168.620776\n",
       " 4      Xdl7t  1.988431  0.155413  4.751769  154.036647\n",
       " ...      ...       ...       ...       ...         ...\n",
       " 99995  DLsed  0.971957  0.370953  6.075346  110.744026\n",
       " 99996  QKivN  1.392429 -0.382606  1.273912  122.346843\n",
       " 99997  3rnvd  1.029585  0.018787 -1.348308   64.375443\n",
       " 99998  7kl59  0.998163 -0.528582  1.583869   74.040764\n",
       " 99999  1CWhH  1.764754 -0.266417  5.722849  149.633246\n",
       " \n",
       " [100000 rows x 5 columns],\n",
       " None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 , df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count   Dtype  \n",
      "---  ------   --------------   -----  \n",
      " 0   id       100000 non-null  object \n",
      " 1   f0       100000 non-null  float64\n",
      " 2   f1       100000 non-null  float64\n",
      " 3   f2       100000 non-null  float64\n",
      " 4   product  100000 non-null  float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 3.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(          id         f0         f1        f2     product\n",
       " 0      kBEdx -15.001348  -8.276000 -0.005876    3.179103\n",
       " 1      62mP7  14.272088  -3.475083  0.999183   26.953261\n",
       " 2      vyE1P   6.263187  -5.948386  5.001160  134.766305\n",
       " 3      KcrkZ -13.081196 -11.506057  4.999415  137.945408\n",
       " 4      AHL4O  12.702195  -8.147433  5.004363  134.766305\n",
       " ...      ...        ...        ...       ...         ...\n",
       " 99995  QywKC   9.535637  -6.878139  1.998296   53.906522\n",
       " 99996  ptvty -10.160631 -12.558096  5.005581  137.945408\n",
       " 99997  09gWa  -7.378891  -3.084104  4.998651  137.945408\n",
       " 99998  rqwUm   0.665714  -6.152593  1.000146   30.132364\n",
       " 99999  relB0  -3.426139  -7.794274 -0.003299    3.179103\n",
       " \n",
       " [100000 rows x 5 columns],\n",
       " None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 , df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count   Dtype  \n",
      "---  ------   --------------   -----  \n",
      " 0   id       100000 non-null  object \n",
      " 1   f0       100000 non-null  float64\n",
      " 2   f1       100000 non-null  float64\n",
      " 3   f2       100000 non-null  float64\n",
      " 4   product  100000 non-null  float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 3.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(          id        f0        f1        f2     product\n",
       " 0      fwXo0 -1.146987  0.963328 -0.828965   27.758673\n",
       " 1      WJtFt  0.262778  0.269839 -2.530187   56.069697\n",
       " 2      ovLUW  0.194587  0.289035 -5.586433   62.871910\n",
       " 3      q6cA6  2.236060 -0.553760  0.930038  114.572842\n",
       " 4      WPMUX -0.515993  1.716266  5.899011  149.600746\n",
       " ...      ...       ...       ...       ...         ...\n",
       " 99995  4GxBu -1.777037  1.125220  6.263374  172.327046\n",
       " 99996  YKFjq -1.261523 -0.894828  2.524545  138.748846\n",
       " 99997  tKPY3 -1.199934 -2.957637  5.219411  157.080080\n",
       " 99998  nmxp2 -2.419896  2.417221 -5.548444   51.795253\n",
       " 99999  V9kWn -2.551421 -2.025625  6.090891  102.775767\n",
       " \n",
       " [100000 rows x 5 columns],\n",
       " None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 , df3.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Upon loading the data, it's visible that there's no missing values from the any of the columns. We can also see that the 'id' column from each dataset is an object dtype. However, based off of the project description, we know that the target of the data is the 'product' column. We know that the other features are important for prediction in regards to a model, but the information from the 'id' column doesn't provide any predictive value and will therefore be excluded from the data from this point forward. The columns remaining all have numerical data types so no encoding is necessary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Predicted Reserves for Region 1: 92.65\n",
      "RMSE:37.90\n",
      "Average Predicted Reserves for Region 2: 69.40\n",
      "RMSE:0.88\n",
      "Average Predicted Reserves for Region 3: 95.10\n",
      "RMSE:40.13\n"
     ]
    }
   ],
   "source": [
    "# splitting and training data\n",
    "# training set will be 75% source data ; validation set will be 25% source data per project instructions\n",
    "\n",
    "#region1\n",
    "region1 = df1.copy()\n",
    "region1.sort_values('product', ascending=False, inplace=True)\n",
    "region1.drop_duplicates('id', keep='first', inplace=True)\n",
    "features_train1, features_valid1, target_train1, target_valid1 = train_test_split(region1[['f0', 'f1', 'f2']], region1['product'].values.reshape(-1, 1), test_size=0.25, random_state=123)\n",
    "model1 = LinearRegression().fit(features_train1, target_train1)    \n",
    "predictions1 = model1.predict(features_valid1)  \n",
    "avg_predictions1 = np.mean(predictions1)\n",
    "region1['predictions'] = model1.predict(region1[['f0', 'f1', 'f2']])\n",
    "print(f'Average Predicted Reserves for Region 1: {avg_predictions1:.2f}')\n",
    "rmse = mean_squared_error(target_valid1, predictions1, squared=False)\n",
    "print(f'RMSE:{rmse:.2f}')\n",
    "\n",
    "#region2\n",
    "region2 = df2.copy()\n",
    "region2.sort_values('product', ascending=False, inplace=True)\n",
    "region2.drop_duplicates('id', keep='first', inplace=True)\n",
    "features_train2, features_valid2, target_train2, target_valid2 = train_test_split(region2[['f0', 'f1', 'f2']], region2['product'].values.reshape(-1, 1), test_size=0.25, random_state=123)\n",
    "model2 = LinearRegression().fit(features_train2, target_train2)    \n",
    "predictions2 = model2.predict(features_valid2)  \n",
    "avg_predictions2 = np.mean(predictions2)\n",
    "region2['predictions'] = model2.predict(region2[['f0', 'f1', 'f2']])\n",
    "print(f'Average Predicted Reserves for Region 2: {avg_predictions2:.2f}')\n",
    "rmse = mean_squared_error(target_valid2, predictions2, squared=False)\n",
    "print(f'RMSE:{rmse:.2f}')\n",
    "\n",
    "\n",
    "#region3\n",
    "region3 = df3.copy()\n",
    "region3.sort_values('product', ascending=False, inplace=True)\n",
    "region3.drop_duplicates('id', keep='first', inplace=True)\n",
    "features_train3, features_valid3, target_train3, target_valid3 = train_test_split(region3[['f0', 'f1', 'f2']], region3['product'].values.reshape(-1, 1), test_size=0.25, random_state=123)\n",
    "model3 = LinearRegression().fit(features_train3, target_train3)    \n",
    "predictions3 = model3.predict(features_valid3)  \n",
    "avg_predictions3 = np.mean(predictions3)\n",
    "region3['predictions'] = model3.predict(region3[['f0', 'f1', 'f2']])\n",
    "print(f'Average Predicted Reserves for Region 3: {avg_predictions3:.2f}')\n",
    "rmse = mean_squared_error(target_valid3, predictions3, squared=False)\n",
    "print(f'RMSE:{rmse:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this information, Region 3 leads in the predicted average volume with Region 2 following closely behind. They both however have a high margin of error which doesn't necessarily instill the most confidence in their numbers. As for Region 2, it has a lower average of reserves available but also has a very low margin of error, suggesting that the numbers being predicted are more than likely what will be found. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing for Profit Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# storing key values for calculations\n",
    "budget = 100e6  # 100 million USD\n",
    "revenue_per_unit = 4.5e3  # 4500 USD per thousand barrels\n",
    "exploration_points = 500\n",
    "development_points = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111.11111111111111"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "break_even = (100_000_000 / 200) / 4500\n",
    "break_even"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that there needs to be a minimum volume of 111 thousand per well to break even and create profit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region 1 - Total Volume: 10076.339907289777, Profit: -54656470.417196006\n",
      "Region 2 - Total Volume: 16171.95661892381, Profit: -27226195.214842856\n",
      "Region 3 - Total Volume: 21258.780355234354, Profit: -4335488.401445404\n"
     ]
    }
   ],
   "source": [
    "def calculate_profit(predictions, target, revenue_per_unit, budget, num_wells):\n",
    "    # Convert to numpy arrays\n",
    "    predictions = np.array(predictions)\n",
    "    target = np.array(target)\n",
    "\n",
    "    # Sort predictions and targets based on highest predicted values\n",
    "    sorted_indices = predictions.argsort()[-num_wells:]\n",
    "    selected_predictions = predictions[sorted_indices]\n",
    "    selected_targets = target[sorted_indices]\n",
    "    \n",
    "    # Calculate total predicted volume\n",
    "    total_volume = selected_targets.sum()\n",
    "    \n",
    "    # Calculate potential profit\n",
    "    revenue = total_volume * revenue_per_unit\n",
    "    profit = revenue - budget\n",
    "    \n",
    "    return profit, total_volume\n",
    "\n",
    "# region1\n",
    "profit1, total_volume1 = calculate_profit(predictions1, target_valid1, revenue_per_unit, budget, development_points)\n",
    "print(f'Region 1 - Total Volume: {total_volume1}, Profit: {profit1}')\n",
    "\n",
    "# region2\n",
    "profit2, total_volume2 = calculate_profit(predictions2, target_valid2, revenue_per_unit, budget, development_points)\n",
    "print(f'Region 2 - Total Volume: {total_volume2}, Profit: {profit2}')\n",
    "\n",
    "# region3\n",
    "profit3, total_volume3 = calculate_profit(predictions3, target_valid3, revenue_per_unit, budget, development_points)\n",
    "print(f'Region 3 - Total Volume: {total_volume3}, Profit: {profit3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the calculation above, it's clear that none of the locations meet the requirements to create profit. Each location's top predictions had a total volume that was much less than the minimum to simply break even. Based on this information though, it's also clear that Region 3 has the greatest amount of volume available and will create the least amount of loss. With that, I would recommend Region 3 as the new location simply because it isn't as damaging to the company as the other two regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Risks and Profit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def bootstrap(df, n=1000, k=500, top=200):\n",
    "    profits = []\n",
    "    \n",
    "    for _ in range(n):\n",
    "        # Sample 500 unique wells\n",
    "        sample = df.sample(k, replace=False)\n",
    "        \n",
    "        # Select top 200 wells based on predictions\n",
    "        top200 = sample.nlargest(top, 'predictions')\n",
    "        \n",
    "        # Calculate profit\n",
    "        profit = top200['product'].sum() * 4500 - 100_000_000\n",
    "        profits.append(profit)\n",
    "    \n",
    "    return np.array(profits)\n",
    "\n",
    "# Apply for each region\n",
    "profits1 = bootstrap(region1)\n",
    "profits2 = bootstrap(region2)\n",
    "profits3 = bootstrap(region3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding average profit, confidence interval, and risk of losses for each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Profit for Region 1: 4246480.195708861\n",
      "95% Confidence Interval: [-843630.84207598 9005833.19901565]\n",
      "Risk of Loss for Region 1: 6.30%\n",
      "Average Profit for Region 2: 4470131.302148249\n",
      "95% Confidence Interval: [ 408848.55448964 8651803.69286311]\n",
      "Risk of Loss for Region 2: 1.50%\n",
      "Average Profit for Region 3: 3578131.7642226564\n",
      "95% Confidence Interval: [-1812405.8972103   8839562.47099121]\n",
      "Risk of Loss for Region 3: 9.60%\n"
     ]
    }
   ],
   "source": [
    "# region1\n",
    "average_profit1 = np.mean(profits1)\n",
    "conf_interval1 = np.percentile(profits1, [2.5, 97.5]) # 95% confidence interval\n",
    "risk_pct1 = (profits1 < 0).mean() * 100\n",
    "\n",
    "print(f'Average Profit for Region 1: {average_profit1}')\n",
    "print(f'95% Confidence Interval: {conf_interval1}')\n",
    "print(f'Risk of Loss for Region 1: {risk_pct1:.2f}%')\n",
    "\n",
    "# region2\n",
    "average_profit2 = np.mean(profits2)\n",
    "conf_interval2 = np.percentile(profits2, [2.5, 97.5])\n",
    "risk_pct2 = (profits2 < 0).mean() * 100\n",
    "\n",
    "print(f'Average Profit for Region 2: {average_profit2}')\n",
    "print(f'95% Confidence Interval: {conf_interval2}')\n",
    "print(f'Risk of Loss for Region 2: {risk_pct2:.2f}%')\n",
    "\n",
    "# region3\n",
    "average_profit3 = np.mean(profits3)\n",
    "conf_interval3 = np.percentile(profits3, [2.5, 97.5])\n",
    "risk_pct3 = (profits3 < 0).mean() * 100\n",
    "\n",
    "print(f'Average Profit for Region 3: {average_profit3}')\n",
    "print(f'95% Confidence Interval: {conf_interval3}')\n",
    "print(f'Risk of Loss for Region 3: {risk_pct3:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon finding the average profit as well as the risk of loss for each region, we can see that Regions 1 and 2 are closer in terms of profit with Region 3 coming in as the least profitable. However, the risk of loss shows that Regions 1 and 3 both are above the threshold wanted by the company, meaning that they have a margin of error too great to be considered for the new development of wells. Region 2 has an average estimated profit of over $4 million. While the confidence innterval does have a large gap from the lowest and highest amounts, the risk of loss is a small amount of 1.5%. So with all of this information, Region 2 is the best location for OilyGiant based on the estimated profit and risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning this project, I was tasked with assisting the mining company OilyGiant with determining which of three locations would be the best place for the development of new wells. When initially preparing the data, it was fairly simple as the data for each region was intact and had no major changes needed. I then moved on to separating into training and validation sets in order to train the data using a Linear Regression model. The predictions on the average amount of volume that each region contained pointed at Region 3 having the most promise. The RMSE values from this analysis was a bit worrisome though, as it suggested a decent margin for error for Regions 1 and 3, whereas Region 2 seemed much more reliable in its numbers. Keeping that in mind, I then moved on to finding information about profit. It was found that each region needed to contain a minimum of just over 11 thousand volumes of oil per well in order to produce a profit for OilyGiant. After some calculation, it was found that none of those regions met that qualification to create a profit. It did let me know that based on the predictions, Region 3 was estimated at having the least amount of loss. The estimated profit (or lack of it) between the three regions did have enough of a distinction that I could only recommend Region 3 as the new location as the numbers had suggested it was going to be the least damaging financially. So I then moved to do some further analysis on profit and risk to see if this was truly the best decision. To do so, I utilized the bootstrapping technique with 1,000 samples to better find the distribution of profit. I also looked for the 95% confidence interval and risk of loss for each region. The threshold for the risk of loss that the new location needed to meet was 2.5%, and only one region met this standard. With this new information in mind, Region 2 is the only reasonable new location for the new developments of wells. This decision was made with the consideration that OilyGiant will make profit from this new region and that based on the RMSE values and risk of loss percentage, they will have security in their decision to invest in this location."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 6,
    "start_time": "2025-04-24T22:05:08.635Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-24T22:05:25.788Z"
   },
   {
    "duration": 302,
    "start_time": "2025-04-24T22:43:33.167Z"
   },
   {
    "duration": 623,
    "start_time": "2025-04-24T22:46:43.874Z"
   },
   {
    "duration": 60,
    "start_time": "2025-04-24T22:46:56.375Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-24T22:49:22.164Z"
   },
   {
    "duration": 99,
    "start_time": "2025-04-24T22:49:25.615Z"
   },
   {
    "duration": 95,
    "start_time": "2025-04-24T22:49:34.110Z"
   },
   {
    "duration": 61,
    "start_time": "2025-04-24T22:51:54.857Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-24T22:51:59.493Z"
   },
   {
    "duration": 58,
    "start_time": "2025-04-24T22:52:05.158Z"
   },
   {
    "duration": 94,
    "start_time": "2025-04-24T22:52:11.055Z"
   },
   {
    "duration": 94,
    "start_time": "2025-04-24T22:55:00.804Z"
   },
   {
    "duration": 59,
    "start_time": "2025-04-24T22:55:42.590Z"
   },
   {
    "duration": 100,
    "start_time": "2025-04-24T22:57:15.087Z"
   },
   {
    "duration": 260,
    "start_time": "2025-04-24T22:59:32.399Z"
   },
   {
    "duration": 273,
    "start_time": "2025-04-24T23:00:09.196Z"
   },
   {
    "duration": 239,
    "start_time": "2025-04-24T23:00:18.808Z"
   },
   {
    "duration": 12,
    "start_time": "2025-04-24T23:00:24.522Z"
   },
   {
    "duration": 11,
    "start_time": "2025-04-24T23:00:27.951Z"
   },
   {
    "duration": 214,
    "start_time": "2025-04-24T23:03:32.771Z"
   },
   {
    "duration": 12,
    "start_time": "2025-04-24T23:03:39.919Z"
   },
   {
    "duration": 15,
    "start_time": "2025-04-24T23:03:56.959Z"
   },
   {
    "duration": 20,
    "start_time": "2025-04-24T23:05:22.640Z"
   },
   {
    "duration": 16,
    "start_time": "2025-04-24T23:05:32.255Z"
   },
   {
    "duration": 324,
    "start_time": "2025-04-26T23:23:27.238Z"
   },
   {
    "duration": 254,
    "start_time": "2025-04-26T23:23:27.564Z"
   },
   {
    "duration": 20,
    "start_time": "2025-04-26T23:23:27.820Z"
   },
   {
    "duration": 14,
    "start_time": "2025-04-26T23:23:27.842Z"
   },
   {
    "duration": 14,
    "start_time": "2025-04-26T23:23:27.858Z"
   },
   {
    "duration": 31,
    "start_time": "2025-04-27T00:05:28.690Z"
   },
   {
    "duration": 31,
    "start_time": "2025-04-27T00:06:27.407Z"
   },
   {
    "duration": 31,
    "start_time": "2025-04-27T00:06:42.438Z"
   },
   {
    "duration": 85,
    "start_time": "2025-04-27T00:07:19.581Z"
   },
   {
    "duration": 288,
    "start_time": "2025-04-27T02:41:39.101Z"
   },
   {
    "duration": 238,
    "start_time": "2025-04-27T02:41:39.391Z"
   },
   {
    "duration": 24,
    "start_time": "2025-04-27T02:41:39.631Z"
   },
   {
    "duration": 15,
    "start_time": "2025-04-27T02:41:39.657Z"
   },
   {
    "duration": 15,
    "start_time": "2025-04-27T02:41:39.675Z"
   },
   {
    "duration": 563,
    "start_time": "2025-04-27T02:51:30.537Z"
   },
   {
    "duration": 273,
    "start_time": "2025-04-27T03:06:18.786Z"
   },
   {
    "duration": 16,
    "start_time": "2025-04-27T03:06:31.115Z"
   },
   {
    "duration": 14,
    "start_time": "2025-04-27T03:06:40.493Z"
   },
   {
    "duration": 16,
    "start_time": "2025-04-27T03:06:50.320Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-27T03:08:46.316Z"
   },
   {
    "duration": 52,
    "start_time": "2025-04-27T03:14:13.288Z"
   },
   {
    "duration": 790,
    "start_time": "2025-04-28T20:10:26.418Z"
   },
   {
    "duration": 247,
    "start_time": "2025-04-28T20:10:27.211Z"
   },
   {
    "duration": 18,
    "start_time": "2025-04-28T20:10:27.460Z"
   },
   {
    "duration": 14,
    "start_time": "2025-04-28T20:10:27.480Z"
   },
   {
    "duration": 12,
    "start_time": "2025-04-28T20:10:27.512Z"
   },
   {
    "duration": 40,
    "start_time": "2025-04-28T20:10:27.525Z"
   },
   {
    "duration": 17,
    "start_time": "2025-04-28T20:30:18.395Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-28T20:30:45.771Z"
   },
   {
    "duration": 46,
    "start_time": "2025-04-28T20:55:22.142Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-28T21:13:15.864Z"
   },
   {
    "duration": 250,
    "start_time": "2025-04-28T21:16:00.723Z"
   },
   {
    "duration": 19,
    "start_time": "2025-04-28T21:16:20.190Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-28T21:16:24.866Z"
   },
   {
    "duration": 110,
    "start_time": "2025-04-28T21:17:29.925Z"
   },
   {
    "duration": 22,
    "start_time": "2025-04-28T21:18:01.273Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-28T21:18:24.967Z"
   },
   {
    "duration": 22,
    "start_time": "2025-04-28T21:18:28.663Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-28T21:18:51.933Z"
   },
   {
    "duration": 14,
    "start_time": "2025-04-28T21:19:06.912Z"
   },
   {
    "duration": 13,
    "start_time": "2025-04-28T21:21:12.855Z"
   },
   {
    "duration": 19,
    "start_time": "2025-04-28T21:22:51.577Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-28T21:48:19.439Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-28T21:51:39.254Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-28T21:51:46.652Z"
   },
   {
    "duration": 17,
    "start_time": "2025-04-28T21:55:36.413Z"
   },
   {
    "duration": 7,
    "start_time": "2025-04-28T21:55:40.965Z"
   },
   {
    "duration": 28,
    "start_time": "2025-04-28T22:03:04.716Z"
   },
   {
    "duration": 25,
    "start_time": "2025-04-28T22:03:22.565Z"
   },
   {
    "duration": 57,
    "start_time": "2025-04-28T22:03:50.772Z"
   },
   {
    "duration": 255,
    "start_time": "2025-04-28T22:05:06.944Z"
   },
   {
    "duration": 16,
    "start_time": "2025-04-28T22:05:10.023Z"
   },
   {
    "duration": 15,
    "start_time": "2025-04-28T22:05:13.744Z"
   },
   {
    "duration": 16,
    "start_time": "2025-04-28T22:05:16.650Z"
   },
   {
    "duration": 44,
    "start_time": "2025-04-28T22:05:36.062Z"
   },
   {
    "duration": 38,
    "start_time": "2025-04-28T22:05:47.073Z"
   },
   {
    "duration": 18,
    "start_time": "2025-04-28T22:05:51.206Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-28T22:14:51.972Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-28T22:14:54.223Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-28T22:17:40.406Z"
   },
   {
    "duration": 734,
    "start_time": "2025-04-28T22:39:15.509Z"
   },
   {
    "duration": 214,
    "start_time": "2025-04-28T22:39:16.245Z"
   },
   {
    "duration": 19,
    "start_time": "2025-04-28T22:39:16.460Z"
   },
   {
    "duration": 14,
    "start_time": "2025-04-28T22:39:16.480Z"
   },
   {
    "duration": 14,
    "start_time": "2025-04-28T22:39:16.512Z"
   },
   {
    "duration": 47,
    "start_time": "2025-04-28T22:39:16.527Z"
   },
   {
    "duration": 43,
    "start_time": "2025-04-28T22:39:16.576Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-28T22:39:16.620Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-28T22:39:16.625Z"
   },
   {
    "duration": 427,
    "start_time": "2025-04-28T23:09:21.934Z"
   },
   {
    "duration": 19,
    "start_time": "2025-04-28T23:12:40.643Z"
   },
   {
    "duration": 74,
    "start_time": "2025-04-28T23:12:47.752Z"
   },
   {
    "duration": 19,
    "start_time": "2025-04-28T23:13:14.171Z"
   },
   {
    "duration": 19,
    "start_time": "2025-04-28T23:15:30.615Z"
   },
   {
    "duration": 59,
    "start_time": "2025-04-28T23:17:39.258Z"
   },
   {
    "duration": 12,
    "start_time": "2025-04-28T23:19:55.003Z"
   },
   {
    "duration": 12,
    "start_time": "2025-04-28T23:34:39.926Z"
   },
   {
    "duration": 60,
    "start_time": "2025-04-29T01:00:39.733Z"
   },
   {
    "duration": 6459,
    "start_time": "2025-04-29T01:02:29.488Z"
   },
   {
    "duration": 6481,
    "start_time": "2025-04-29T01:02:59.459Z"
   },
   {
    "duration": 6466,
    "start_time": "2025-04-29T01:03:14.909Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-29T01:08:00.535Z"
   },
   {
    "duration": 157,
    "start_time": "2025-04-29T03:05:06.480Z"
   },
   {
    "duration": 762,
    "start_time": "2025-04-29T03:05:10.257Z"
   },
   {
    "duration": 289,
    "start_time": "2025-04-29T03:05:13.359Z"
   },
   {
    "duration": 19,
    "start_time": "2025-04-29T03:05:24.817Z"
   },
   {
    "duration": 15,
    "start_time": "2025-04-29T03:05:34.271Z"
   },
   {
    "duration": 18,
    "start_time": "2025-04-29T03:05:43.873Z"
   },
   {
    "duration": 242,
    "start_time": "2025-04-29T03:13:56.510Z"
   },
   {
    "duration": 95,
    "start_time": "2025-04-29T03:15:14.400Z"
   },
   {
    "duration": 78,
    "start_time": "2025-04-29T03:16:51.510Z"
   },
   {
    "duration": 191,
    "start_time": "2025-04-29T03:18:13.027Z"
   },
   {
    "duration": 221,
    "start_time": "2025-04-29T03:20:22.192Z"
   },
   {
    "duration": 60,
    "start_time": "2025-04-29T03:20:39.819Z"
   },
   {
    "duration": 242,
    "start_time": "2025-04-29T03:22:59.298Z"
   },
   {
    "duration": 239,
    "start_time": "2025-04-29T03:25:41.015Z"
   },
   {
    "duration": 165,
    "start_time": "2025-04-29T03:25:49.356Z"
   },
   {
    "duration": 598,
    "start_time": "2025-04-29T03:28:19.307Z"
   },
   {
    "duration": 180,
    "start_time": "2025-04-29T03:28:57.539Z"
   },
   {
    "duration": 97,
    "start_time": "2025-04-29T03:32:27.199Z"
   },
   {
    "duration": 97,
    "start_time": "2025-04-29T03:34:43.540Z"
   },
   {
    "duration": 49,
    "start_time": "2025-04-29T03:35:45.819Z"
   },
   {
    "duration": 203,
    "start_time": "2025-04-29T03:38:18.430Z"
   },
   {
    "duration": 280,
    "start_time": "2025-04-29T03:40:14.862Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-29T03:43:39.829Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-29T03:44:14.938Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-29T03:44:23.485Z"
   },
   {
    "duration": 168,
    "start_time": "2025-04-29T03:50:12.960Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-29T03:50:56.344Z"
   },
   {
    "duration": 6,
    "start_time": "2025-04-29T03:55:19.278Z"
   },
   {
    "duration": 69,
    "start_time": "2025-04-29T04:05:10.673Z"
   },
   {
    "duration": 6782,
    "start_time": "2025-04-29T04:06:57.171Z"
   },
   {
    "duration": 6701,
    "start_time": "2025-04-29T04:07:04.405Z"
   },
   {
    "duration": 326,
    "start_time": "2025-04-29T04:08:17.619Z"
   },
   {
    "duration": 6775,
    "start_time": "2025-04-29T04:08:36.468Z"
   },
   {
    "duration": 7,
    "start_time": "2025-04-29T04:08:57.086Z"
   },
   {
    "duration": 7,
    "start_time": "2025-04-29T04:22:45.257Z"
   },
   {
    "duration": 9,
    "start_time": "2025-04-29T04:23:02.322Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
