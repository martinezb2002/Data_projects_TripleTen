{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is in conjuction with Zyfra, a company whose goal is to develop efficiency solutions for heavy industry. This project specifically will be for gold mining. Prior to beginning this project, I had to become familiar with the process of gold extraction and the different stages and substances I would soon be dealing with. Once familiar, I'll begin by loading and preparing the data, making it ready for analysis. I will then find different metrics that give insight into the process of various stages and purifications. This will give insight into what areas of recovery may be more valuable than others. I'll finish by testing different models with the target of predicting the amount of gold recovered from gold ore. By doing this, I hope to help maximize production efforts while also eliminating unprofitable parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries \n",
    "import pandas as pd \n",
    "from sklearn.metrics import mean_absolute_error \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/datasets/gold_recovery_train.csv')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/datasets/gold_recovery_test.csv')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.read_csv('/datasets/gold_recovery_full.csv')\n",
    "full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon loading the data and finding some general info, each dataframe seems to be well structured and without any issues. I will then proceed with some analysis, first by determining if recovery is calculated correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now analyze the features that are not available in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzing against the training set as the full set is a combination of both and won't separate needed contexts\n",
    "features_only_in_train = set(train.columns) - set(test.columns)\n",
    "features_only_in_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the dtypes of the features above \n",
    "for feature in features_only_in_train:\n",
    "    print(f'{feature}: {train[feature].dtype}') \n",
    "\n",
    "# also finding the number of features \n",
    "f'Number of features only in training: {len(features_only_in_train)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completing data preprocessing. Will begin by merging the test and full set in order to prepare for analysis and model testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging test and full set \n",
    "test_copy = test.copy()\n",
    "test_merged = test_copy.merge(full[['date']+list(features_only_in_train)], how='right', on='date')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(train.columns) - set(test_merged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged[['rougher.output.recovery', 'final.output.recovery']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only feature that seems to be in the wrong dtype is the 'date' column in both the test and training sets. For now, I'll go ahead and remove that feature as well as the index column as it holds little to no weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged = test_merged.drop('date', axis=1) \n",
    "train = train.drop('date', axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicates\n",
    "test_merged.columns.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicated columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.duplicated().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop_duplicates().reset_index(drop=True) \n",
    "test_merged = test_merged.drop_duplicates().reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No more duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As some of the missing values are in features that may hold a lot of weight towards the target, I will be filling them in to maintain the size of the datasets as well as the integrity of the process data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged = test_merged.fillna(test_merged.median())\n",
    "train = train.fillna(train.median()) # filling with the median as it deals better in terms of outliers and some features may be of importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying changes\n",
    "test_merged.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_merged = test_merged.drop('date', axis=1)\n",
    "# train = train.drop('index', axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding recovery calculation\n",
    "c = train['rougher.output.concentrate_au']\n",
    "f = train['rougher.input.feed_au']\n",
    "t = train['rougher.output.tail_au']\n",
    "\n",
    "recovery_calculation = (c*(f-t))/(f*(c-t))*100 \n",
    "\n",
    "# calculating the MAE\n",
    "valid_data = train.dropna(\n",
    "    subset=[\n",
    "        'rougher.output.recovery', \n",
    "        'rougher.output.concentrate_au', \n",
    "        'rougher.input.feed_au', \n",
    "        'rougher.output.tail_au'\n",
    "    ]\n",
    ")\n",
    "valid_data['rougher.output.recovery'] = valid_data['rougher.output.recovery'].astype(np.float32) \n",
    "recovery_calculation = recovery_calculation.replace([np.inf, -np.inf], np.nan).dropna() \n",
    "valid_data = valid_data.loc[recovery_calculation.index] \n",
    "mae = mean_absolute_error(valid_data['rougher.output.recovery'], recovery_calculation) \n",
    "\n",
    "f'Mean Absolute Error of Recovery Calculation: {mae}' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is indicating a prediction difference of about 57 units from the actual recovery values. This will be interesting when looking at the MAE from the machine learning models later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning the target and feature variables\n",
    "target_train = train[['rougher.output.recovery', 'final.output.recovery']]\n",
    "features_train = train.drop(['rougher.output.recovery', 'final.output.recovery'], axis=1) \n",
    "\n",
    "target_test = test_merged[['rougher.output.recovery', 'final.output.recovery']] \n",
    "features_test = test_merged.drop(['rougher.output.recovery', 'final.output.recovery'], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged['total_raw_feed'] = test_merged[['rougher.input.feed_au', 'rougher.input.feed_ag', 'rougher.input.feed_pb']].sum(axis=1)\n",
    "test_merged['total_rougher_concentrate'] = test_merged[['rougher.output.concentrate_au', 'rougher.output.concentrate_ag', 'rougher.output.concentrate_pb']].sum(axis=1)\n",
    "test_merged['total_final_concentrate'] = test_merged[['final.output.concentrate_au', 'final.output.concentrate_ag', 'final.output.concentrate_pb']].sum(axis=1) \n",
    "\n",
    "train['total_raw_feed'] = train[['rougher.input.feed_au', 'rougher.input.feed_ag', 'rougher.input.feed_pb']].sum(axis=1)\n",
    "train['total_rougher_concentrate'] = train[['rougher.output.concentrate_au', 'rougher.output.concentrate_ag', 'rougher.output.concentrate_pb']].sum(axis=1)\n",
    "train['total_final_concentrate'] = train[['final.output.concentrate_au', 'final.output.concentrate_ag', 'final.output.concentrate_pb']].sum(axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting how the concentrations of metals change depending on the purification stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metal_stages = [\n",
    "    'rougher.output.concentrate', \n",
    "    'primary_cleaner.output.concentrate', \n",
    "    'secondary_cleaner.output.concentrate',\n",
    "    'final.output.concentrate'\n",
    "]\n",
    "   \n",
    "metals = ['au', 'ag', 'pb']\n",
    "\n",
    "for stage in metal_stages:\n",
    "    for metal in metals:\n",
    "        column_name = f'{stage}_{metal}'\n",
    "        if column_name in train.columns:\n",
    "            print(f'{column_name}: {train[column_name].mean()}')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metal_stages = [\n",
    "    'rougher.output.concentrate',\n",
    "    'primary_cleaner.output.concentrate',\n",
    "    'secondary_cleaner.output.concentrate',\n",
    "    'final.output.concentrate'\n",
    "]\n",
    "\n",
    "metals = ['au', 'ag', 'pb']\n",
    "\n",
    "# plot\n",
    "for metal in metals:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for stage in metal_stages:\n",
    "        column_name = f'{stage}_{metal}'\n",
    "        \n",
    "        if column_name in train.columns:\n",
    "            plt.hist(train[column_name], bins=30, alpha=0.5, label=stage)\n",
    "    \n",
    "    plt.title(f'Distribution of {metal.upper()} Concentration')\n",
    "    plt.xlabel('Concentration')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As purification progresses, the different metals (gold, silver, lead) each do different things. Just looking at the averages, the concentration of gold is the highest of the three and it continues to increase to more than double that amount by the end of the purification process. On the other hand, silver goes down in concentration and lead grows, but generally stays within the same amount. The visuals then give more insight into the fact that at the beginning of the process, there is a much higher amount of lead and silver present when compared to gold. By the end of the process, however, the presence of gold is much more apparent. The visuals also represent the information derived from the previous analysis which provided the averages of the same prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing feed particle size distributions in the training set and testing set\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(train['rougher.input.feed_size'], color='blue', label='Train', kde=True, stat=\"density\", linewidth=0)\n",
    "sns.histplot(test['rougher.input.feed_size'], color='red', label='Test', kde=True, stat=\"density\", linewidth=0)\n",
    "\n",
    "plt.title('Feed Particle Size Distribution')\n",
    "plt.xlabel('Particle Size')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the feed particle size distributions in the training and test set. With the visual it's clear to see that the distributions are in no way significantly different, suggesting that the model evaluation will reflect accurate information and will thus be correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histograms for each stage\n",
    "stages = ['total_raw_feed', 'total_rougher_concentrate', 'total_final_concentrate']\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "for i, stage in enumerate(stages, 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.hist(train[stage], bins=30, color='skyblue', edgecolor='black')\n",
    "    plt.title(stage)\n",
    "    plt.xlabel('Total Concentration')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visual above represents the total concentration of substances at different stages of the recovery process. Here we can see that in the beginning of the process, there is a much larger total concentration compared to the next stage and the last one as well. This makes sense as the substances go through purification to have a concentration of only gold by the end of it. We can also see where there are abnormal values, outliers, where there's almost no peaks in the plots. While it makes sense that there are peaks climbing high, it doesn't make sense to have no amount of substances. With that in mind, if these values stay in the set, they may cause confusion and inaccurate findings in the future so it is best to have them removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing near_zero values \n",
    "threshold = 1.0 # removes values that may be an error or irrelevant while maintaining as much integrity as possible\n",
    "\n",
    "# remove from training set \n",
    "train = train[\n",
    "    (train['total_raw_feed'] > threshold) &\n",
    "    (train['total_rougher_concentrate'] > threshold) &\n",
    "    (train['total_final_concentrate'] > threshold)\n",
    "]\n",
    "\n",
    "# remove from testing set\n",
    "test_merged = test_merged[\n",
    "    (test_merged['total_raw_feed'] > threshold) &\n",
    "    (test_merged['total_rougher_concentrate'] > threshold) &\n",
    "    (test_merged['total_final_concentrate'] > threshold)\n",
    "] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the type of data we are handling (continuous), the models that will be tested will be regression models since the target variable is not a binary outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the sMAPE function to use later once the values are available from the model testings\n",
    "def smape(y_true, y_pred):\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train)\n",
    "test_scaled = scaler.transform(test_merged) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training various models and determining which performs the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = test.columns.intersection(features_train.columns)\n",
    "features_train_aligned = features_train[common_columns]\n",
    "features_test_aligned = features_test[common_columns] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_aligned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest model \n",
    "model_rf_multi = MultiOutputRegressor(RandomForestRegressor(random_state=123, n_estimators=10))\n",
    "model_rf_multi.fit(features_train_aligned, target_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "cv_predictions = cross_val_predict(model_rf_multi, features_train_aligned, target_train, cv=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating sMAPE \n",
    "smape_rougher = smape(target_train['rougher.output.recovery'], cv_predictions[:, 0])\n",
    "smape_final = smape(target_train['final.output.recovery'], cv_predictions[:, 1]) \n",
    "smape_value = 0.25 * smape_rougher + 0.75 * smape_final\n",
    "f'sMAPE: {smape_value}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality metrics \n",
    "\n",
    "# first target\n",
    "mae_rf_rougher = mean_absolute_error(target_train['rougher.output.recovery'], cv_predictions[:, 0])\n",
    "rmse_rf_rougher = mean_squared_error(target_train['rougher.output.recovery'], cv_predictions[:, 0], squared=False)\n",
    "\n",
    "# second target\n",
    "mae_rf_final = mean_absolute_error(target_train['final.output.recovery'], cv_predictions[:, 1])\n",
    "rmse_rf_final = mean_squared_error(target_train['final.output.recovery'], cv_predictions[:, 1], squared=False)\n",
    "\n",
    "print(f'MAE Rougher: {mae_rf_rougher}, MAE Final: {mae_rf_final}')\n",
    "print(f'RMSE Rougher: {rmse_rf_rougher}, RMSE Final: {rmse_rf_final}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both metrics indicate prediction accuracy, however there may be some outliers present. These in addition with the sMAPE value will be compared with another model to see which performs best with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression model \n",
    "model_lr_multi = MultiOutputRegressor(LinearRegression())\n",
    "model_lr_multi.fit(features_train_aligned, target_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "cv_predictions_lr = cross_val_predict(model_lr_multi, features_train_aligned, target_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating sMAPE \n",
    "smape_rougher_lr = smape(target_train['rougher.output.recovery'], cv_predictions_lr[:, 0])\n",
    "smape_final_lr = smape(target_train['final.output.recovery'], cv_predictions_lr[:, 1]) \n",
    "smape_value_lr = 0.25 * smape_rougher_lr + 0.75 * smape_final_lr\n",
    "f'sMAPE: {smape_value_lr}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality metrics \n",
    "# first target\n",
    "mae_rf_rougher_lr = mean_absolute_error(target_train['rougher.output.recovery'], cv_predictions_lr[:, 0])\n",
    "rmse_rf_rougher_lr = mean_squared_error(target_train['rougher.output.recovery'], cv_predictions_lr[:, 0], squared=False)\n",
    "\n",
    "# second target\n",
    "mae_rf_final_lr = mean_absolute_error(target_train['final.output.recovery'], cv_predictions_lr[:, 1])\n",
    "rmse_rf_final_lr = mean_squared_error(target_train['final.output.recovery'], cv_predictions_lr[:, 1], squared=False)\n",
    "\n",
    "print(f'MAE Rougher: {mae_rf_rougher_lr}, MAE Final: {mae_rf_final_lr}')\n",
    "print(f'RMSE Rougher: {rmse_rf_rougher_lr}, RMSE Final: {rmse_rf_final_lr}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the values from both models, we can see that the Random Forest Regressor has lower values in all metrics tested. These indicate that this model performs better in terms of percentage error as well as accuracy. We can now move forward with final testing with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'estimator__n_estimators': [50, 100],\n",
    "    'estimator__max_depth': [10, 20],\n",
    "} \n",
    "\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
    "    denominator[denominator == 0] = 1  # Prevent division by zero\n",
    "    smape_value = 100 * np.mean(2 * np.abs(y_pred - y_true) / denominator)\n",
    "    return smape_value\n",
    "\n",
    "def multi_target_smape(y_true, y_pred):\n",
    "    y_true_rougher , y_pred_rougher = y_true[0], y_pred[0] \n",
    "    y_true_final, y_pred_final = y_true[1], y_pred[1] \n",
    "    smape_value_lr = 0.25 * smape(y_true_rougher, y_pred_rougher) + 0.75 * smape(y_true_final, y_pred_final) \n",
    "    return smape_value_lr \n",
    "\n",
    "smape_scorer = make_scorer(multi_target_smape, greater_is_better=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train_array = target_train.values if hasattr(target_train, 'values') else target_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 1000 rows randomly\n",
    "sample_size = 1000\n",
    "\n",
    "# randomly select\n",
    "sample_indices = target_train.sample(n=sample_size, random_state=42).index\n",
    "\n",
    "# create samples \n",
    "sample_train = features_train.loc[sample_indices]\n",
    "sample_target = target_train.loc[sample_indices]\n",
    "\n",
    "# convert to arrays\n",
    "sample_train_array = sample_train.values\n",
    "sample_target_array = sample_target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(MultiOutputRegressor(RandomForestRegressor(random_state=123)), param_grid, scoring=smape_scorer, cv=3, n_jobs=-1, verbose=2)\n",
    "   \n",
    "grid_search.fit(sample_train_array, sample_target_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best sMAPE Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is performing at a high rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final testing\n",
    "final_model = MultiOutputRegressor(RandomForestRegressor(random_state=123, n_estimators=best_params['estimator__n_estimators'], max_depth=best_params['estimator__max_depth']))\n",
    "final_model.fit(features_train_aligned, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "final_predictions = final_model.predict(features_test_aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(y_true, y_pred):\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_smape = multi_target_smape(target_test.values, final_predictions)\n",
    "print(f'Final sMAPE: {final_smape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final metrics \n",
    "mae_rougher_test = mean_absolute_error(target_test['rougher.output.recovery'], final_predictions[:, 0])\n",
    "rmse_rougher_test = mean_squared_error(target_test['rougher.output.recovery'], final_predictions[:, 0], squared=False)\n",
    "\n",
    "mae_final_test = mean_absolute_error(target_test['final.output.recovery'], final_predictions[:, 1])\n",
    "rmse_final_test = mean_squared_error(target_test['final.output.recovery'], final_predictions[:, 1], squared=False)\n",
    "\n",
    "print(f'MAE Rougher Test: {mae_rougher_test}, MAE Final Test: {mae_final_test}')\n",
    "print(f'RMSE Rougher Test: {rmse_rougher_test}, RMSE Final Test: {rmse_final_test}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During final testing, I evaluated the same metrics as before when finding a model to train. Thankfully, these metrics are the best ones so far, meaning the model is performing at a high level. Accuracy and predictions are at its highest so far. Let's do some checks to see more information on how the model is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate means for constant predictions\n",
    "mean_rougher = target_train['rougher.output.recovery'].mean()\n",
    "mean_final = target_train['final.output.recovery'].mean()\n",
    "\n",
    "# create constant predictions based on means\n",
    "constant_predictions = np.full((len(target_test), 2), [mean_rougher, mean_final])\n",
    "\n",
    "# calculate sMAPE for constant model\n",
    "smape_rougher_constant = smape(target_test['rougher.output.recovery'], constant_predictions[:, 0])\n",
    "smape_final_constant = smape(target_test['final.output.recovery'], constant_predictions[:, 1])\n",
    "smape_constant_value = 0.25 * smape_rougher_constant + 0.75 * smape_final_constant\n",
    "\n",
    "# print constant model sMAPE\n",
    "print(f'sMAPE Constant Model: {smape_constant_value}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at feature importance\n",
    "importance = final_model.estimators_[0].feature_importances_\n",
    "plt.barh(range(len(importance)), importance)\n",
    "plt.yticks(range(len(importance)), features_train_aligned.columns)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visual is important as the goal of this project is to eliminate wasteful parameters. Here, we can see the features that the model relies on more than others. While these features make sense as to why they're given a bit more weight, it's interesting to see how little others are contributing, especially since the model is predicting so well already. These features may contribute to overfitting and if removed can improve the model even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = target_test['rougher.output.recovery'] - final_predictions[:, 0]\n",
    "sns.histplot(residuals, bins=20, kde=True)\n",
    "plt.title('Residuals for Rougher')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above visual shows the distribution of prediction for the final model. The peak being at zero suggests that most predictions are indeed accurate. The symmetry of the plot also suggests that the model is fairly predicting both positive and negtive errors. Then looking at the sMAPE value from the sanity check, we can see that it is significantly larger than the sMAPE value from the final model. So in terms of capturing complexity and variability, the final model captures the data's complexity well. Keeping all of this in mind, the final model is currently predicting with high accuracy and low bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this project was to reduce wasteful parameters, optimize production, and find a model that would best predict the amount of gold recovered from gold ore. Loading the data went without problem as there were no major issues to address. Immediately, I took a look at the recovery calculation and found that it suggested extreme accuracy to the real values. I then moved on to examining the differences between the test and training set (which was helpful information when performing model testing). Here I found a difference in the amount of columns. While at this point, I also looked into duplicates, missing values, and dtypes. I merged the full set with the test set to gain targets while also removing unnecessary features. Once the data was complete and ready for analysis, I looked at the concentration of metals at various stages of purification. I found that gold increased with each subsequent stage while silver went down, and lead essentially stayed around the same amount. Keeping in mind the purpose of the process, this information made sense. I also examined the feed particle distribution between the training and test set in preparation for model training. Thankfully, the values were matching up, giving security that the values received from the models would be accurate. I also looked at the total concentrations of substances at different stages. This showed us that in the beginning of the recovery process is when there's the most amount of substances present. Then the amount decreases as the stages progress, but the concentration also increases. Thinking back to the last visual, this makes sense. The gold increases while the others decrease. I moved on to conclude this project with model training. Based on the type of data we are working with (continuous), only regression models would be of use as they would not be predicting targets that are binary. I began with a RandomForestRegressor and then moved on to train a LinearRegression model as well. I examined quality metrics consisting of the sMAPE value, MAE, and RMSE. With the RandomForestRegressor model, all of the metrics tested came in at better quality with the RandomForestRegressor. Then with final testing, we achieved the lowest scores yet, suggesting that the model is performing at a high quality and is extremely accurate. The feature importances and residuals then showed which features contribute most to this success and which may be contributing to overfitting. A great majority of the features towards the end of the recovery process are contributing very little (if at all) towards the model's total performance. The sanity check showed that the final model is performing at a high rate, handling variability and patterns well. With the information about the various metals, the various stages, and the performance of the model,  we will most definitely be able to improve the way gold is recovered from gold ore from a business perspective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 291,
    "start_time": "2025-05-04T15:16:54.103Z"
   },
   {
    "duration": 190,
    "start_time": "2025-05-04T15:18:12.132Z"
   },
   {
    "duration": 384,
    "start_time": "2025-05-04T15:18:16.000Z"
   },
   {
    "duration": 249,
    "start_time": "2025-05-04T15:18:30.232Z"
   },
   {
    "duration": 198,
    "start_time": "2025-05-04T15:19:03.711Z"
   },
   {
    "duration": 308,
    "start_time": "2025-05-04T15:19:31.484Z"
   },
   {
    "duration": 224,
    "start_time": "2025-05-04T15:19:35.852Z"
   },
   {
    "duration": 220,
    "start_time": "2025-05-04T15:20:01.147Z"
   },
   {
    "duration": 281,
    "start_time": "2025-05-04T15:20:12.325Z"
   },
   {
    "duration": 295,
    "start_time": "2025-05-11T20:59:15.522Z"
   },
   {
    "duration": 260,
    "start_time": "2025-05-11T20:59:15.819Z"
   },
   {
    "duration": 209,
    "start_time": "2025-05-11T20:59:16.080Z"
   },
   {
    "duration": 316,
    "start_time": "2025-05-11T20:59:16.291Z"
   },
   {
    "duration": 288,
    "start_time": "2025-05-11T21:09:13.966Z"
   },
   {
    "duration": 344,
    "start_time": "2025-05-11T21:24:37.747Z"
   },
   {
    "duration": 213,
    "start_time": "2025-05-11T21:24:49.747Z"
   },
   {
    "duration": 149,
    "start_time": "2025-05-11T21:24:59.560Z"
   },
   {
    "duration": 150,
    "start_time": "2025-05-11T21:26:14.678Z"
   },
   {
    "duration": 162,
    "start_time": "2025-05-11T21:27:16.384Z"
   },
   {
    "duration": 338,
    "start_time": "2025-05-11T22:34:34.686Z"
   },
   {
    "duration": 315,
    "start_time": "2025-05-11T22:34:35.026Z"
   },
   {
    "duration": 196,
    "start_time": "2025-05-11T22:34:35.343Z"
   },
   {
    "duration": 261,
    "start_time": "2025-05-11T22:34:35.541Z"
   },
   {
    "duration": 179,
    "start_time": "2025-05-11T22:34:35.809Z"
   },
   {
    "duration": 360,
    "start_time": "2025-05-11T22:34:35.991Z"
   },
   {
    "duration": 197,
    "start_time": "2025-05-11T22:34:36.353Z"
   },
   {
    "duration": 499,
    "start_time": "2025-05-11T23:42:49.589Z"
   },
   {
    "duration": 24,
    "start_time": "2025-05-11T23:45:21.746Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
